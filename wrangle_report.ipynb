{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "#### By: [Samuel Duah Boadi](https://www.linkedin.com/in/samuel-duah-boadi-8ab46944/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data wrangling is the process of gathering data, assessing the data’s quality and structure before cleaning it. In this project, all the three steps in data wrangling, that is, gathering, assessing and cleaning.\n",
    "\n",
    "This report describes the wrangling efforts done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "In the first step, gathering data: the three datasets were collected through three different methods.\n",
    "\n",
    "> **Download manually**<br/>\n",
    "The twitter_archive_enhanced.csv was acquired by manually downloading the file through the link provided. Once the file was downloaded and kept in the appropriate folder it was read into a pandas dataframe, naming it as twitter_arch.\n",
    "\n",
    "> **Download programmatically**<br/>\n",
    "The file image_predictions.tsv which was hosted on the Udacity’s servers was downloaded programmatically using the request library and read into a pandas dataframe, naming it as image_pred.\n",
    "\n",
    "> **API**<br/>\n",
    "An additional data containing retweet count and favorite count of each tweets in the twitter archive was needed. To get that data, I used the tweet IDs in the archive to query the Twitter API using tweepy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "The second step was assessing the quality and structure of the three dataframes. I scrolled through the datasets visually which was in some cases not effective. Searching for issues programmatically using code was helpful. \n",
    "\n",
    "Some functions in pandas that was used to assess the datasets were shape to see the number of columns and observations, info(), value_counts(), duplicated() and describe(). \n",
    "\n",
    "Issues identified were documented at the bottom of the Assessing Data section. The issues were grouped into two; Quality issues and Tidiness issues.\n",
    "\n",
    "#### Quality issues\n",
    "> `twitter_arch` table<ul>\n",
    "    <li>tweet_id is an integer not a string</li>       \n",
    "    <li>in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id and retweeted_status_user_id are floats not strings</li>\n",
    "    <li>timestamp and retweeted_status_timestamp are strings not datetimes</li>\n",
    "    <li>doggo, floofer, pupper and puppo are strings not booleans</li>\n",
    "    <li>name column has wrong dog names</li>\n",
    "    <li>Some columns in the dataset not needed for the analysis</li>\n",
    "    <li>A row has rating_denominator equal to 0</li>\n",
    "</ul>\n",
    "\n",
    "> `image_pred` table<ul>\n",
    "    <li>tweet_id is an integer not a string</li>\n",
    "    <li>Underscore (_) between words in p1, p2 and p3 columns</li>\n",
    "    <li>Inconsistent case in p1, p2 and p3 columns</li>\n",
    "</ul>\n",
    "\n",
    "> `tweet_json` table<ul>\n",
    "    <li>tweet_id is an integer and not string</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "#### Tidiness issues\n",
    "<ul>\n",
    "    <li>`tweet_json` table should be part of the `twitter_arch` table</li>\n",
    "    <li>`image_pred` table should be part of the `twitter_arch` table</li>\n",
    "    <li>Three columns (doggo, pupper and puppo) instead of one column 'dog_stage</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "The final step in Data Wrangling is cleaning. In this step, all of the issues documented while assessing would be cleaned. Before the cleaning, a copy of the original data was made in order not to make changes to the original data. \n",
    "\n",
    "The programmatic data cleaning process was followed, that is, define, code and test.<br/>\n",
    "How to clean the issue was defined then converted the definition into executable code and finally test the data to ensure the code was implemented correctly.\n",
    "\n",
    "The cleaned dataset was save to csv file named twitter_archive_master.csv\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "Based on the definition, dogs were classified either as doggo, pupper or puppo. Floofer is any dog with 'seemingly excess fur'. Some observation had multiple dog stages (doggo, pupper, puppo and floofer). It was imposiible to create a 'dog_stage' column stating only one stage of the dog.\n",
    "\n",
    "A number of tweets did not include the names of the dog, hence prompted the use of 'None'in those cases.\n",
    "\n",
    "The image classifier predicted some images as not dog when in some cases there were actually dogs in the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
